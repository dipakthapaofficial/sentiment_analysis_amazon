{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748c5f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets\n",
    "\n",
    "from file_parser import getFiles, getCsvFiles\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import time\n",
    "import dill\n",
    "\n",
    "#Import joblib to host ml model\n",
    "# import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7726f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeScore(data):\n",
    "    if data < 3:\n",
    "        return -1\n",
    "    elif data > 3:\n",
    "        return 1\n",
    "    elif data == 3:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fb326a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(fileName):\n",
    "#     df_train_dataset = pd.read_json(fileName, lines = True)\n",
    "    df_train_dataset = pd.read_csv(fileName)\n",
    "    print(df_train_dataset.shape)\n",
    "    \n",
    "    #remove rows with empty reviews\n",
    "    df_train_dataset = df_train_dataset[df_train_dataset['reviewText'].notna()]\n",
    "    df_train_dataset = df_train_dataset[df_train_dataset['reviewText'] != \"\"]\n",
    "    df_train_dataset['overall'] = df_train_dataset['overall'].astype(object) # fix datatype error\n",
    "    \n",
    "    df_train_dataset['reviewText'] = df_train_dataset['reviewText'].astype(object) # fix datatype error\n",
    "    \n",
    "    #Add only useful data from dataset\n",
    "    dataset = {\"reviewText\": df_train_dataset[\"reviewText\"], \"overall\": df_train_dataset[\"overall\"]  }\n",
    "    \n",
    "    df_train_dataset = pd.DataFrame(data = dataset)\n",
    "    df_train_dataset = df_train_dataset.dropna()\n",
    "    \n",
    "    print(df_train_dataset.shape)\n",
    "\n",
    "#     df_train_dataset['score'] = df_train_dataset[\"overall\"].apply(lambda x: computeScore(x))\n",
    "\n",
    "    #Filter out neutral i.e. rating 3 from the list to segregate between positive and negative reviews\n",
    "    df_train_dataset[\"score\"] = df_train_dataset[\"overall\"].apply(lambda rating : +1 if str(rating) > '3' else -1)\n",
    "\n",
    "    #Get shape of the data frame\n",
    "    print(df_train_dataset.shape)\n",
    "\n",
    "    #Print data frame from top\n",
    "    print(df_train_dataset.head())\n",
    "    \n",
    "    #df_train_dataset.to_csv(fileName, index=False)\n",
    "    return df_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d11767a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_pool \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# fileNames = getFiles(\"data\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m fileNames \u001b[38;5;241m=\u001b[39m getCsvFiles(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "data_pool = pd.DataFrame()  \n",
    "    \n",
    "    \n",
    "# fileNames = getFiles(\"data\")\n",
    "fileNames = getCsvFiles(\"data/final\")\n",
    "\n",
    "for fileName in fileNames:\n",
    "    dataset = classifier(fileName)\n",
    "    data_pool = data_pool.append(dataset, ignore_index = True)\n",
    "\n",
    "data_pool.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a317d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = data_pool['score'].value_counts()\n",
    "print(x)\n",
    "sns.barplot(x.index,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pool.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec6bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. WORD-COUNT\n",
    "data_pool['word_count'] = data_pool['reviewText'].apply(lambda x: len(str(x).split()))\n",
    "print(data_pool[data_pool['score'] == -1]['word_count'].mean()) #Negative Reviews\n",
    "print(data_pool[data_pool['score'] == 1]['word_count'].mean()) #Positive Reviews\n",
    "# print(data_pool[data_pool['score'] == 0]['word_count'].mean()) #Neutral Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. CHARACTER-COUNT\n",
    "data_pool['char_count'] = data_pool['reviewText'].apply(lambda x: len(str(x)))\n",
    "print(data_pool[data_pool['score'] == -1]['char_count'].mean()) #Negative Reviews\n",
    "print(data_pool[data_pool['score'] == 1]['char_count'].mean()) #Positive Reviews\n",
    "# print(data_pool[data_pool['score'] == 0]['char_count'].mean()) #Neutral Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98391278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. UNIQUE WORD-COUNT\n",
    "data_pool['unique_word_count'] = data_pool['reviewText'].apply(lambda x: len(set(str(x).split())))\n",
    "print(data_pool[data_pool['score'] == -1]['unique_word_count'].mean()) #Negative Reviews\n",
    "print(data_pool[data_pool['score'] == 1]['unique_word_count'].mean()) #Positive Reviews\n",
    "# print(data_pool[data_pool['score'] == 0]['unique_word_count'].mean()) #Neutral Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting word-count per review\n",
    "fig,(ax1,ax2)=plt.subplots(1, 2, figsize=(500,100))\n",
    "\n",
    "train_words = data_pool[data_pool['score'] == 1]['word_count']\n",
    "ax1.hist(train_words,color='green')\n",
    "ax1.set_title('Positive Reviews')\n",
    "\n",
    "train_words=data_pool[data_pool['score'] == -1]['word_count']\n",
    "ax2.hist(train_words,color='red')\n",
    "ax2.set_title('Negative Reviews')\n",
    "\n",
    "# train_words=data_pool[data_pool['score'] == 0]['word_count']\n",
    "#ax3.hist(train_words,color='blue')\n",
    "#ax3.set_title('Neutral Reviews')\n",
    "\n",
    "fig.suptitle('Words per review')\n",
    "plt.show()\n",
    "\n",
    "# fig, ax3 = plt.subplots(1, 2)\n",
    "# train_words=data_pool[data_pool['score'] == 0]['word_count']\n",
    "# ax3.hist(train_words, color='blue')\n",
    "# ax3.set_title('Neutral Reviews')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01d48d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned it may involve some things like adjacent spaces and tabs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#1. Common text preprocessing\n",
    "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\n",
    "\n",
    "#convert to lowercase and remove punctuations and characters and then strip\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = text.lower()#lowercase text\n",
    "    text=text.strip() #get rid of leading/trailing whitespace \n",
    "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful\n",
    "    text = re.sub('\\s+', ' ', text)   #Remove extra space and tabs\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)   #[0-9] matches any digit (0 to 10000...)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip()) \n",
    "    text = re.sub(r'\\d',' ',text)  #matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text)   #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n",
    "    \n",
    "    return text\n",
    "\n",
    "text=preprocess(text)\n",
    "print(text)  #text is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390fc810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messag clean may involv thing like adjac space tab\n",
      "messag clean may involv thing like adjac space tab\n",
      "messag clean may involv thing like adjac space tab\n"
     ]
    }
   ],
   "source": [
    "#3. LEXICON-BASED TEXT PROCESSING EXAMPLES\n",
    " \n",
    "#1. STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "text=stopword(text)\n",
    "print(text)\n",
    "\n",
    "#2. STEMMING\n",
    " \n",
    "# Initialize the stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "def stemming(string):\n",
    "    a=[snow.stem(i) for i in word_tokenize(string) ]\n",
    "    return \" \".join(a)\n",
    "text=stemming(text)\n",
    "print(text)\n",
    "\n",
    "#3. LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "# def lemmatizer(string, cores=6):  \n",
    "#     a = []\n",
    "    \n",
    "#     with Pool(processes=cores) as pool:\n",
    "#         # Initialize the lemmatizer\n",
    "#         wl = WordNetLemmatizer()\n",
    "#         word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "#         a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    \n",
    "#     return \" \".join(a)\n",
    "\n",
    "text = lemmatizer(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL PREPROCESSING\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "    return stopword(preprocess(string))\n",
    "\n",
    "# data_pool=data_pool.drop(columns=['word_count','char_count','unique_word_count'])\n",
    "data_pool['clean_text'] = data_pool['reviewText'].apply(lambda x: finalpreprocess(x))\n",
    "data_pool.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eaaf0a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Export cleaned up data-pool for future usage. skipping previous steps\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdata_pool\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_up_data_jupyter_with_clean_text.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_pool' is not defined"
     ]
    }
   ],
   "source": [
    "#Export cleaned up data-pool for future usage. skipping previous steps\n",
    "data_pool.to_csv('cleaned_up_data_jupyter_with_clean_text.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4874bda7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/final/abc/xyz/Appliances_final.csv', '../data/final/abc/xyz/AMAZON_FASHION.csv', '../data/final/abc/xyz/Prime_Pantry_final.csv', '../data/final/abc/xyz/All_Beauty.csv', '../data/final/abc/xyz/Industrial_and_Scientific_final.csv', '../data/final/abc/xyz/Digital_Music_final.csv', '../data/final/abc/xyz/Luxury_Beauty_final.csv', '../data/final/abc/xyz/Magazine_Subscriptions_final.csv', '../data/final/abc/xyz/Software_final.csv', '../data/final/abc/xyz/Gift_Cards.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
      "/tmp/ipykernel_157996/1482890640.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6937302, 4)\n",
      "                                          reviewText  overall  score  \\\n",
      "0  Not one thing in this book seemed an obvious o...        5      1   \n",
      "1  I have enjoyed Dr. Alan Gregerman's weekly blo...        5      1   \n",
      "2  Alan Gregerman believes that innovation comes ...        5      1   \n",
      "3  Alan Gregerman is a smart, funny, entertaining...        5      1   \n",
      "4  As I began to read this book, I was again remi...        5      1   \n",
      "\n",
      "                                          clean_text  \n",
      "0  one thing book seem obvious original think how...  \n",
      "1  enjoy dr alan gregerman weekly blog two previo...  \n",
      "2  alan gregerman believe innovation come view pr...  \n",
      "3  alan gregerman smart funny entertain writer jo...  \n",
      "4  begin read book remind deceptively complicate ...  \n",
      "Total time to load clean data= 45.32081651687622\n"
     ]
    }
   ],
   "source": [
    "# data_pool.head()\n",
    "# fileNames = getCsvFiles(\"final\")\n",
    "# data_pool = pd.DataFrame()\n",
    "# for fileName in fileNames:\n",
    "#     data_pool_classified = pd.read_csv(fileName)\n",
    "#     data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
    "\n",
    "start_time = time.time()\n",
    "data_pool = pd.DataFrame()\n",
    "fileNames = getCsvFiles('../data/final/abc/xyz')\n",
    "for fileName in fileNames:\n",
    "    data_pool_classified = pd.read_csv(fileName)\n",
    "    data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
    "    \n",
    "\n",
    "print(data_pool.shape)\n",
    "print(data_pool.head())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time to load clean data=\",(end_time-start_time))\n",
    "\n",
    "# dill.dump_session('notebook_env_data_load.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1466ef22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                 reviewText  overall  score  \\\n",
       "0        Not one thing in this book seemed an obvious o...        5      1   \n",
       "1        I have enjoyed Dr. Alan Gregerman's weekly blo...        5      1   \n",
       "2        Alan Gregerman believes that innovation comes ...        5      1   \n",
       "3        Alan Gregerman is a smart, funny, entertaining...        5      1   \n",
       "4        As I began to read this book, I was again remi...        5      1   \n",
       "...                                                    ...      ...    ...   \n",
       "6937297  I always enjoy getting these Gift cards via em...        5      1   \n",
       "6937298                                       Worked great        4      1   \n",
       "6937299                                          Gift card        5      1   \n",
       "6937300            What is there to say, It's a gift card.        5      1   \n",
       "6937301  No complaints, ordered it and within 5 or so m...        5      1   \n",
       "\n",
       "                                                clean_text  \n",
       "0        one thing book seem obvious original think how...  \n",
       "1        enjoy dr alan gregerman weekly blog two previo...  \n",
       "2        alan gregerman believe innovation come view pr...  \n",
       "3        alan gregerman smart funny entertain writer jo...  \n",
       "4        begin read book remind deceptively complicate ...  \n",
       "...                                                    ...  \n",
       "6937297  always enjoy get gift card via email lowe home...  \n",
       "6937298                                         work great  \n",
       "6937299                                          gift card  \n",
       "6937300                                      say gift card  \n",
       "6937301  complaint order within minute receive email co...  \n",
       "\n",
       "[6923651 rows x 4 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pool.isna().sum()\n",
    "\n",
    "data_pool = data_pool[data_pool['clean_text'].notna()]\n",
    "data_pool = data_pool[data_pool['reviewText'].notna()]\n",
    "\n",
    "data_pool.isna().sum()\n",
    "data_pool.shape\n",
    "data_pool.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e02b10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.43856564481178\n",
      "34.18957296926893\n"
     ]
    }
   ],
   "source": [
    "#1. WORD-COUNT\n",
    "data_pool['word_count'] = data_pool['reviewText'].apply(lambda x: len(str(x).split()))\n",
    "print(data_pool[data_pool['score'] == -1]['word_count'].mean()) #Negative Reviews\n",
    "print(data_pool[data_pool['score'] == 1]['word_count'].mean()) #Positive Reviews\n",
    "# print(data_pool[data_pool['score'] == 0]['word_count'].mean()) #Neutral Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084a64b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277.50777474283024\n",
      "185.29145849173216\n"
     ]
    }
   ],
   "source": [
    "#2. CHARACTER-COUNT\n",
    "data_pool['char_count'] = data_pool['reviewText'].apply(lambda x: len(str(x)))\n",
    "print(data_pool[data_pool['score'] == -1]['char_count'].mean()) #Negative Reviews\n",
    "print(data_pool[data_pool['score'] == 1]['char_count'].mean()) #Positive Reviews\n",
    "# print(data_pool[data_pool['score'] == 0]['char_count'].mean()) #Neutral Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb09c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.662020161117326\n",
      "26.76033350169632\n"
     ]
    }
   ],
   "source": [
    "#3. UNIQUE WORD-COUNT\n",
    "data_pool['unique_word_count'] = data_pool['reviewText'].apply(lambda x: len(set(str(x).split())))\n",
    "print(data_pool[data_pool['score'] == -1]['unique_word_count'].mean()) #Negative Reviews\n",
    "print(data_pool[data_pool['score'] == 1]['unique_word_count'].mean()) #Positive Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a98516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2AAAAILCAYAAACO1v+JAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAr9ElEQVR4nO3de7ivZV0n/vcnwDOJys4UUGhCGzITf9tTmpKdwExmJivQSieVbLJfXtpBq5+nJhuz6ZeWpXgeDfBQFpmGTilqKLHxFIdIAhxAjZ2oeMgD+pk/nmfDcrHW3gvc6157fdfrdV3r2s/hXvf63N/9Zd+8v8/9PKu6OwAAAKy/b9roAgAAALYKAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQyAm6yqnlVVr93oOtZTVb21qh6z0XUAsBgEMIAFUlVPr6q3Ljv2kVWOnTC2us2pu4/r7ldvdB0ALAYBDGCxvCvJ91TVfklSVXdKckCSo5cd+/a57ZpV1f57udZv2Fpq2hfrBmDrEsAAFss5mQLXveb9703yjiQXLTv2L939saq6c1WdXlVXV9XFVfWEXR3NywvfWFWvraprkjy2qo6oqjOr6rNV9fYkBy9pf4u57Ser6tNVdU5V3XGlIqvqsvlq3QVV9amqemVV3WLJ+YdX1Qfnfs6qqnsu+95fq6oPJ/n8SgGrqrqqfqGqPpLkI7vrc+7rjcu+/wVV9cJ5+51V9fgl5362qi6c6z6jqu46H392Vf3hvH1AVX2+qp4/79+yqr5YVbdf7S8OgK1BAANYIN395SRnJ3nwfOjBSd6d5D3Lju26+nVakiuS3DnJI5M8t6oeuqTL45O8MclBSf40ySlJzs0UvH4rydJ7ox6T5LZJDktyhyRPTPLvuyn30Ul+OMl/SHK3JL+ZJFV1dJJXJPm5uZ+XJDm9qm6+5HtPTPIjSQ7q7mtX6f8/JblfkqP20OdpSR5WVQfOP3+/JD8xj/XrVNXxSX49yX9Jsi3Ta3vqfPrMJMfM2/dJ8olc/5o/IMlF3X31bl4PALYAAQxg8ZyZ6//H/3szhYR3Lzt2ZlUdluSBSX6tu7/Y3R9M8rIkP7Okr/d2919099cyBY77JPn/uvtL3f2uJH+1pO1XMoWbb+/ur3b3ud19zW7q/KPuvnwOJb+dKVQlyUlJXtLdZ8/9vDrJl5Lcf8n3vnD+3t0FvN/p7qvnNqv22d0fTfL+JP95/r6HJvlCd79vhT6fOPd74Rz8npvkXvNVsPcmObKq7pDptX55kkOq6jZJHpLp7wWALU4AA1g870ryoHm527bu/kiSszLdG3b7JPeY29w5ydXd/dkl3/vRJIcs2b98yfadk3yquz+/rP0ur0lyRpLTqupjVfW7VXXAbupc2vdH5/6T5K5JnjovFfx0VX0601W1O6/yvWvpf099npLrA+CjssLVryX9vGBJH1cnqSSHzEFvR6aw9eBMgeusTCFXAAMgiQAGsIjem2kp4BOS/H2SzFeiPjYf+1h3Xzrv337X0rvZXZJcuWS/l2x/PMntqurWy9pn/hlf6e5nd/dRSb4nycPz9VfTljtsWT8fm7cvT/Lb3X3Qkq9bdfepS9ovrWs1S9vsqc83JDmmqg7NdCVstQB2eZKfW9bPLbv7rPn8mZmuoB2d6X68MzMts7xvbuRDTwBYTAIYwIJZciXmKZmWHu7ynvnYu+Z2l2e6QvM78wM07pnkcUlW/L1e81K9HUmeXVU3q6oHJfnRXeer6vuq6rvme6iuybQk8Wu7KfUXqurQ+arcbyR53Xz8pUmeWFX3q8mtq+pHlgXFG2u3fXb3ziTvTPLKJJd294Wr9PPiJE+vqu+cx3zbqvrxJefPzBQ6L5jvx3tnksfPfe78BuoHYEEIYACL6cwk35IpdO3y7vnY0isxJyY5PNPVpzcleWZ3/+/d9PuoTA+2uDrJM5P8ryXnvjXTAzuuSXLhXMNrdtPXKUneluSSJP+S5L8nSXfvyHSl7o+SfCrJxUkeu5t+9miNfZ6S5Aey+tWvdPebkjwv0zLLa5Kcl+S4JU3OSnLLXP8aX5Dki3H1C4BZda9lFQcA7D1VdVmSx+8h7AHAwnEFDAAAYBABDAAAYBBLEAEAAAZxBQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAQwAAGAQAYyFVFW/XlUv2835R1fV20bW9I3YbPUCsDiq6q1V9ZiNrmOtNlu9bD3V3RtdA6SqLktyxyRfTfL5JG9N8qTu/txe6PvwJJcmOaC7r/1G+9vDz3pVkkcl+fL8dW6SX+zuf1rPnwvA4pjnxFslOaK7Pz8fe3ySn+ruY9b5Zz8rybd390+t58+Zf1Yn+UKSTvKZJK9L8ivd/dX1/tmwkVwBY1/yo919myT3TrI9yW9ucD031e/O4zgkyZVJXr7B9QCw+eyX5Jc2uogBvnueMx+S5CeT/OwG1wPrTgBjn9PdV2a6AnaPJKmqR1TV+VX16ap6Z1X9x11tq+rXqurKqvpsVV1UVd8/H39WVb12bvau+c9PV9XnquoBVfXYqnrP3PZPqur3ltZQVX9ZVU+Zt+9cVX9WVTur6tKq+n/XOI5/T/L6JPda0u+Kfc3H/72qbr+k7dFV9W9VdcDSeudz31FVb6+qq+dx/8R8/Ij5dfqmef+lVXXVku97TVU9ed5+bFVdMr92l1bVo9cyLgCGeH6SX66qg1Y6udo8MJ+7Q1X9VVVdU1XnVNV/XzaHvKCqLp/Pn1tV3zsfPzbJryf5yXm+/NB8/J1V9fiquvk8x9xjSV/b5vnrW+b9h1fVB+d2Z1XVPdcy2O6+OMnf5+vnzBX7muf+Ny57PV5QVS9cWu+Scz9bVRdW1aeq6oyquut8/NlV9Yfz9gFV9fmqev68f8uq+mJV3b6qblFVr62qT861nFNVd1zLuGAlAhj7nKo6LMnDknygqu6W5NQkT06yLclbkvxVVd2squ6e5ElJ7tPdByb54SSXrdDlg+c/D+ru23T3e5edPzXTZFPzz79dkh9KctocZP4qyYcyXdH6/iRPrqofXsM4bp3kxCQXz/ur9tXdH0vy3iQ/tqSLRyV5Y3d/ZYV+357klCTfkuSEJH9cVUd196VJrkly9JKxf66uD60PSXLm3McLkxw3v3bfk+SDexoTAMPsSPLOJL+8/MTu5oG5yYsyLef/1iSPmb+WOidT0Ln93McbquoW3f03SZ6b5HXzfPndS7+pu7+U5M8zzW27/ESSM7v7qqo6OskrkvxckjskeUmS06vq5nsabFV9R5LvzfVz5u76Oi3Jw6rqwLntfnMdp6zQ7/GZQuV/yfT/Ee/ONO8nyZlJjpm375PkE7n+/xkekOSi7r460+t32ySHzbU8Mcm/72lMsJp1C2BV9Yqquqqqzltj+5+oqgtqutJxg/+A2BL+oqo+neQ9mf5RfG6m5Qh/3d1vn4PI7yW5ZabA8NUkN09yVFUd0N2Xdfe/3ISf++5M68+/d95/ZJL3zqHoPkm2dfdzuvvL3X1JkpdmmuxW88vzOD6b5EFJfno+vqe+Tsk8qc1h8ISsMJkkeXiSy7r7ld19bXd/IMmfJfnx+fyZSR5SVd86779x3j8iyTdnCoBJ8rUk96iqW3b3x7v7/D28TsBeYH7kRnhGkl+sqm3Ljq86D8xh5MeSPLO7v9DdFyR59dJv7u7Xdvcn5+/9n5nm0ruvsaZT8vVz4KNy/Vx1UpKXdPfZ3f3V7n51ki8luf9u+nt/VX0+yYWZAucf76mv7v5okvcn+c9z24cm+UJ3v2+F/p+Y5He6+8L5PvDnJrnXfBXsvUmOrKo7ZApeL09ySFXtWhJ55tzHVzIFr2+fazm3u6/Z0wsFq1nPK2CvSnLsWhpW1ZFJnp7kgd39nZmudrD1/KfuPqi779rd/21ewnfnJB/d1aC7v5bk8iSHzMsVnpzkWUmuqqrTqurON/aH9vQkmtNy/Sd6j0ryp/P2XZPceV5y8Ok5WP16pgeGrOb3uvugJIdn+oRs16S2p77+LMkDqupOmSaCr2UKh8vdNcn9lvXz6EyfdCbXf6L34EzLL9+ZaSJ5SJJ3d/fX5pu6fzLTxPTxqvrr+dNHYP29KuZH1qC7z0vy5iRPW3Zqd/PAtiT7Z5ord1m6nar65XlJ3mfm771tkoPXWNY7ktyqqu5X00Ou7pXkTUvqeuqyug7LNJev5t5JbpNpTrpfkluvsa/rPrTM14fA5e6a5AVL+rg6SWX6/4h/z3Sl8SGZ5swzk5yV5IH5+gD2miRnZFoZ87Gq+t2qOmA3Y4LdWrcA1t3vyvQmv05V/Yeq+pua1hu/e8n/8D0hyYu6+1Pz914VmHws0z+eSa67MnRYpodbpLtP6e4HzW06yfNW6GMtj/o8Nckj50/E7pcpDCXTpHXpHAx3fR3Y3Q/bU4fd/X8y3UD9gqq65Z76mt//b8s0CT0qyWm98mNKL8+03GNpP7fp7p+fz5+Z6WreMfP2e3LDySTdfUZ3/2CSOyX5p0xX44B1Zn7kRnpmpvfBIUuO7W4e2Jnk2iSHLml/2K6Nmu73+tVMS/ZuN39g+JlMoSTZw5w5P6Hw9ZnCz4lJ3tzdn11S128vq+tW3X3qav3NfXZ3vz7TFalnrLGvNyQ5pqoOzXQlbLUAdnmSn1vWzy27+6z5/JmZrqAdnWlp5pmZbmm4b+Z7yLv7K9397O4+KtMKnIcn+ZndjQl2Z/Q9YCdneiT3/5NpTfOuy8x3S3K3qvr7qnpfTTeBQjL9I/8jVfX986dNT820BOGsqrp7VT10Xg/+xUxXm762Qh875+PfttoPmZdv/FuSlyU5o7s/PZ/6hySfremG31tW1X5VdY+qus9aiu/ut2cKkSetsa9TMv2j/sisPpm8OdN/Lz9d003DB1TVfXbd59XdH5lfi5/KNEFfk+RfMy1JOTNJquqOVXX8fB/Bl5J8Liu/dsAY5kdWNK/2eF2SpQ+AWnUemAPSnyd5VlXdag7zS8PCgZkC2s4k+1fVMzItT9/lX5McXvPDnFZxSqYPCx+dr5+rXprkifPVsaqqW1fVj9R8r9Ya/I8kT5iX0O+2r+7emWmFxyszfbh54Sp9vjjJ06vqO5Okqm5bVT++5PyZmV6fC7r7y3Ofj5/73Dl/z/dV1XfNyzuvybQk0ZzJTTYsgM3rab8n042eH8x0M+Wd5tP7Jzky0yf2JyZ5aa3y1B+2lu6+KFOQ+MNMAelHMz2u/suZ1qz/j/n4JzLdiPz0Ffr4QpLfTvL38xKE1dain5LkB7JkMpknsodnWmJxaa4Pabe9EcN4fqZPG/dfQ1+nZ/pv4RPd/aGsYP6k8YcyrcH/WKaxPy/T67HLmUk+2d2XL9mvTGvmk+m//afM3391pqtjPx9gOPMja/CcXL80by3zwJMyzS2fyLR87tRMH7Yl01K6v0nyz5mW+H8xX79E8Q3zn5+sqvdnBd19dqaHfNw501OLdx3fkelq3R8l+VSmB2o8dq2D7O5/zHTV6VfW2NcN5u0V+nxTptfmtKq6Jsl5SY5b0uSsTPeW73pi8gWZXpN3LWnzrZnup74m071qZ2Z6XeEmWddfxDyvDX5zd9+jqr4509Nk7rRCuxcnObu7Xznv/22Sp3X3OetWHABsEPMjI1XV85J8a3cvfxoisAGGXQGbl0Fduuuy73w5edfjTf8i82NAq+rgTEsuLhlVGwBsFPMje1tNvyPsnvN76b5JHpfrH5QBbLD1fAz9qZluprx7VV1RVY/LtFb4cTX9Yr/zkxw/Nz8j06XuCzI9XedXuvuT61UbAGwU8yMDHJjpPrDPZ7p/7H8m+csNrQi4zrouQQQAAOB6o5+CCAAAsGXtvx6dHnzwwX344YevR9cA7EPOPffcf+vubRtdx2ZhfgTYOlabI9clgB1++OHZsWPHenQNwD6kqj660TVsJuZHgK1jtTnSEkQAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBBBDAAAIBB9t/oAlZTz66NLiFJ0s/sjS4BAK5X+8b8mCRpcyTAjeUKGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAAwCACGAAsU1WvqKqrquq8Vc4/uqo+XFX/WFVnVdV3j64RgM1JAAOAG3pVkmN3c/7SJA/p7u9K8ltJTh5RFACb3/4bXQAA7Gu6+11Vdfhuzp+1ZPd9SQ5d96IAWAiugAHAN+ZxSd662smqOqmqdlTVjp07dw4sC4B9kQAGADdRVX1fpgD2a6u16e6Tu3t7d2/ftm3buOIA2CetaQliVV2W5LNJvprk2u7evp5FAcC+rqrumeRlSY7r7k9udD0AbA435h6w7+vuf1u3SgBgk6iquyT58yQ/3d3/vNH1ALB5eAgHACxTVacmOSbJwVV1RZJnJjkgSbr7xUmekeQOSf64qhKrQwBYo7UGsE7ytqrqJC/p7hs8breqTkpyUpLc5S532XsVAsBg3X3iHs4/PsnjB5UDwAJZ60M4HtTd905yXJJfqKoHL2/gJmMAAIDdW1MA6+4r5z+vSvKmJPddz6IAAAAW0R4DWFXduqoO3LWd5IeSnLfehQEAACyatdwDdsckb5pvMt4/ySnd/TfrWhUAAMAC2mMA6+5Lknz3gFoAAAAW2lofwgEAAMA3SAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADgGWq6hVVdVVVnbfK+aqqF1bVxVX14aq69+gaAdicBDAAuKFXJTl2N+ePS3Lk/HVSkj8ZUBMAC0AAA4BluvtdSa7eTZPjk/yvnrwvyUFVdacx1QGwmQlgAHDjHZLk8iX7V8zHbqCqTqqqHVW1Y+fOnUOKA2DfJYABwDrq7pO7e3t3b9+2bdtGlwPABhPAAODGuzLJYUv2D52PAcBuCWAAcOOdnuRn5qch3j/JZ7r74xtdFAD7vv03ugAA2NdU1alJjklycFVdkeSZSQ5Iku5+cZK3JHlYkouTfCHJf92YSgHYbAQwAFimu0/cw/lO8guDygFggViCCAAAMIgABgAAMIgABgAAMIgABgAAMIgABgAAMIgABgAAMIgABgAAMMiaA1hV7VdVH6iqN69nQQAAAIvqxlwB+6UkF65XIQAAAItuTQGsqg5N8iNJXra+5QAAACyutV4B+4Mkv5rka6s1qKqTqmpHVe3YuXPn3qgNAABgoewxgFXVw5Nc1d3n7q5dd5/c3du7e/u2bdv2WoEAAACLYi1XwB6Y5BFVdVmS05I8tKpeu65VAQAALKA9BrDufnp3H9rdhyc5IcnfdfdPrXtlAAAAC8bvAQMAABhk/xvTuLvfmeSd61IJAADAgnMFDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDAAAYBABDABWUFXHVtVFVXVxVT1thfN3qap3VNUHqurDVfWwjagTgM1FAAOAZapqvyQvSnJckqOSnFhVRy1r9ptJXt/dRyc5Ickfj60SgM1IAAOAG7pvkou7+5Lu/nKS05Icv6xNJ/nmefu2ST42sD4ANqn9N7oAANgHHZLk8iX7VyS537I2z0rytqr6xSS3TvIDY0oDYDNzBQwAbpoTk7yquw9N8rAkr6mqG8yrVXVSVe2oqh07d+4cXiQA+xYBDABu6Mokhy3ZP3Q+ttTjkrw+Sbr7vUlukeTg5R1198ndvb27t2/btm2dygVgsxDAAOCGzklyZFUdUVU3y/SQjdOXtfk/Sb4/SarqP2YKYC5xAbBbAhgALNPd1yZ5UpIzklyY6WmH51fVc6rqEXOzpyZ5QlV9KMmpSR7b3b0xFQOwWXgIBwCsoLvfkuQty449Y8n2BUkeOLouADY3V8AAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAG2WMAq6pbVNU/VNWHqur8qnr2iMIAAAAWzf5raPOlJA/t7s9V1QFJ3lNVb+3u961zbQAAAAtljwGsuzvJ5+bdA+avXs+iAAAAFtGa7gGrqv2q6oNJrkry9u4+e4U2J1XVjqrasXPnzr1cJgAAwOa3pgDW3V/t7nslOTTJfavqHiu0Obm7t3f39m3btu3lMgEAADa/G/UUxO7+dJJ3JDl2XaoBAABYYGt5CuK2qjpo3r5lkh9M8k/rXBcAAMDCWctTEO+U5NVVtV+mwPb67n7z+pYFAACweNbyFMQPJzl6QC0AAAAL7UbdAwYAAMBNJ4ABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABAAAMIoABwAqq6tiquqiqLq6qp63S5ieq6oKqOr+qThldIwCbz/4bXQAA7Guqar8kL0ryg0muSHJOVZ3e3RcsaXNkkqcneWB3f6qqvmVjqgVgM3EFDABu6L5JLu7uS7r7y0lOS3L8sjZPSPKi7v5UknT3VYNrBGATEsAA4IYOSXL5kv0r5mNL3S3J3arq76vqfVV17EodVdVJVbWjqnbs3LlzncoFYLMQwADgptk/yZFJjklyYpKXVtVByxt198ndvb27t2/btm1shQDscwQwALihK5MctmT/0PnYUlckOb27v9Ldlyb550yBDABWJYABwA2dk+TIqjqiqm6W5IQkpy9r8xeZrn6lqg7OtCTxkoE1ArAJCWAAsEx3X5vkSUnOSHJhktd39/lV9ZyqesTc7Iwkn6yqC5K8I8mvdPcnN6ZiADYLj6EHgBV091uSvGXZsWcs2e4kT5m/AGBNXAEDAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYRAADAAAYZI8BrKoOq6p3VNUFVXV+Vf3SiMIAAAAWzf5raHNtkqd29/ur6sAk51bV27v7gnWuDQAAYKHs8QpYd3+8u98/b382yYVJDlnvwgAAABbNjboHrKoOT3J0krNXOHdSVe2oqh07d+7cS+UBAAAsjjUHsKq6TZI/S/Lk7r5m+fnuPrm7t3f39m3btu3NGgEAABbCmgJYVR2QKXz9aXf/+fqWBAAAsJjW8hTESvLyJBd29++vf0kAAACLaS1XwB6Y5KeTPLSqPjh/PWyd6wIAAFg4e3wMfXe/J0kNqAUAAGCh3ainIAIAAHDTCWAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAAACDCGAAsIKqOraqLqqqi6vqabtp92NV1VW1fWR9AGxOAhgALFNV+yV5UZLjkhyV5MSqOmqFdgcm+aUkZ4+tEIDNSgADgBu6b5KLu/uS7v5yktOSHL9Cu99K8rwkXxxZHACblwAGADd0SJLLl+xfMR+7TlXdO8lh3f3Xu+uoqk6qqh1VtWPnzp17v1IANhUBDABupKr6piS/n+Spe2rb3Sd39/bu3r5t27b1Lw6AfZoABgA3dGWSw5bsHzof2+XAJPdI8s6quizJ/ZOc7kEcAOyJAAYAN3ROkiOr6oiqulmSE5Kcvutkd3+muw/u7sO7+/Ak70vyiO7esTHlArBZCGAAsEx3X5vkSUnOSHJhktd39/lV9ZyqesTGVgfAZrb/RhcAAPui7n5LkrcsO/aMVdoeM6ImADY/V8AAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAGEcAAAAAG2WMAq6pXVNVVVXXeiIIAAAAW1VqugL0qybHrXAcAAMDC22MA6+53Jbl6QC0AAAALba/dA1ZVJ1XVjqrasXPnzr3VLQAAwMLYawGsu0/u7u3dvX3btm17q1sAAICF4SmIAAAAgwhgAAAAg6zlMfSnJnlvkrtX1RVV9bj1LwsAAGDx7L+nBt194ohCAAAAFp0liAAAAIMIYAAAAIMIYACwgqo6tqouqqqLq+ppK5x/SlVdUFUfrqq/raq7bkSdAGwuAhgALFNV+yV5UZLjkhyV5MSqOmpZsw8k2d7d90zyxiS/O7ZKADYjAQwAbui+SS7u7ku6+8tJTkty/NIG3f2O7v7CvPu+JIcOrhGATUgAA4AbOiTJ5Uv2r5iPreZxSd660omqOqmqdlTVjp07d+7FEgHYjAQwAPgGVNVPJdme5Pkrne/uk7t7e3dv37Zt29jiANjn7PH3gAHAFnRlksOW7B86H/s6VfUDSX4jyUO6+0uDagNgE3MFDABu6JwkR1bVEVV1syQnJDl9aYOqOjrJS5I8oruv2oAaAdiEBDAAWKa7r03ypCRnJLkwyeu7+/yqek5VPWJu9vwkt0nyhqr6YFWdvkp3AHAdSxABYAXd/ZYkb1l27BlLtn9geFEAbHqugAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAwigAEAAAyypgBWVcdW1UVVdXFVPW29iwKAjbanua+qbl5Vr5vPn11Vh29AmQBsMnsMYFW1X5IXJTkuyVFJTqyqo9a7MADYKGuc+x6X5FPd/e1J/v8kzxtbJQCb0f5raHPfJBd39yVJUlWnJTk+yQXrWdi+op5dG13CdfqZvdElAGwVa5n7jk/yrHn7jUn+qKqqu7fOP9a178yR2UIvO7C5rSWAHZLk8iX7VyS53/JGVXVSkpPm3c9V1UXfYG0HJ/m3b7CPzWDN46xn7UMT3Y3n73OxGOdi+UbGede9Wcg+ZC1z33VtuvvaqvpMkjtk2WtpftyrVh/7vhQG18dW/XvfquNOtu7YF2ncK86Rawlga9LdJyc5eW/1V1U7unv73upvX2Wci8U4F4txsjeYH/ceY996Y9+q40627ti3wrjX8hCOK5MctmT/0PkYACyqtcx917Wpqv2T3DbJJ4dUB8CmtZYAdk6SI6vqiKq6WZITkpy+vmUBwIZay9x3epLHzNuPTPJ3W+r+LwBukj0uQZzXtT8pyRlJ9kvyiu4+f90r24vLNfZxxrlYjHOxGOcWtdrcV1XPSbKju09P8vIkr6mqi5NcnSmkjbCV/76MfevZquNOtu7YF37c5cM6AACAMdb0i5gBAAD4xglgAAAAg+xzAayqjq2qi6rq4qp62kbXc2NV1Suq6qqqOm/JsdtX1dur6iPzn7ebj1dVvXAe64er6t5Lvucxc/uPVNVjVvpZG6mqDquqd1TVBVV1flX90nx8ocZaVbeoqn+oqg/N43z2fPyIqjp7Hs/r5pv0U1U3n/cvns8fvqSvp8/HL6qqH96gIe1WVe1XVR+oqjfP+ws3zqq6rKr+sao+WFU75mML9b5Nkqo6qKreWFX/VFUXVtUDFnGcW81mnyNXslXmzeW2yjy63FabV1eyFebalWyV+XdNunuf+cp0o/O/JPm2JDdL8qEkR210XTdyDA9Ocu8k5y059rtJnjZvPy3J8+bthyV5a5JKcv8kZ8/Hb5/kkvnP283bt9vosS0b552S3HvePjDJPyc5atHGOtd7m3n7gCRnz/W/PskJ8/EXJ/n5efu/JXnxvH1CktfN20fN7+ebJzlifp/vt9HjW2G8T0lySpI3z/sLN84klyU5eNmxhXrfzjW+Osnj5+2bJTloEce5lb6yAHPkKuPaEvPmCuPeEvPoCuPeUvPqKq/Bws+1q4z7smyB+XdNr8VGF7DsL+EBSc5Ysv/0JE/f6LpuwjgOXzaRXJTkTvP2nZJcNG+/JMmJy9slOTHJS5Yc/7p2++JXkr9M8oOLPNYkt0ry/iT3y/Qb2vefj1/3vs30xLQHzNv7z+1q+Xt5abt95SvT7zn62yQPTfLmue5FHOdKE8BCvW8z/T6qSzM/aGlRx7nVvrIgc+QqYzs8W2zeXOE1WPh5dIUxL/S8usqYt8Rcu8rYL8uCz79r/drXliAekuTyJftXzMc2uzt298fn7U8kueO8vdp4N9XrMF8SPzrTp1gLN9Z5qcAHk1yV5O2ZPmn6dHdfOzdZWvN145nPfybJHbIJxpnkD5L8apKvzft3yGKOs5O8rarOraqT5mOL9r49IsnOJK+cl7m8rKpuncUb51azlf4+ttR7ddHn0eW20Ly6kj/I1phrV7IV5t812dcC2MLrKa73Rtext1TVbZL8WZInd/c1S88tyli7+6vdfa9Mn1rdN8l3bGxFe19VPTzJVd197kbXMsCDuvveSY5L8gtV9eClJxfkfbt/piVdf9LdRyf5fKalHddZkHGyBSz6e3UrzKPLbYV5dSVbbK5dyVaYf9dkXwtgVyY5bMn+ofOxze5fq+pOSTL/edV8fLXxborXoaoOyDRp/Gl3//l8eCHHmiTd/ekk78i0POCgqtr1i8yX1nzdeObzt03yyez743xgkkdU1WVJTsu0NOIFWbxxpruvnP+8KsmbMk3+i/a+vSLJFd199rz/xkyBbNHGudVspb+PLfFe3Wrz6HILPq+uZMvMtSvZIvPvmuxrAeycJEfOT4O5WaYbDk/f4Jr2htOTPGbefkymdd67jv/M/KSX+yf5zHwZ9owkP1RVt5ufBvND87F9RlVVkpcnubC7f3/JqYUaa1Vtq6qD5u1bZlqff2GmCeORc7Pl49w1/kcm+bv5E53Tk5wwP9HoiCRHJvmHIYNYg+5+encf2t2HZ/rv7u+6+9FZsHFW1a2r6sBd25neb+dlwd633f2JJJdX1d3nQ9+f5IIs2Di3oEWdI1ey8O/VrTKPLrdV5tWVbJW5diVbZf5ds42+CW35V6annvxzpvXAv7HR9dyE+k9N8vEkX8n0KfTjMq3X/dskH0nyv5Pcfm5bSV40j/Ufk2xf0s/PJrl4/vqvGz2uFcb5oEyXiT+c5IPz18MWbaxJ7pnkA/M4z0vyjPn4t2X6x+7iJG9IcvP5+C3m/Yvn89+2pK/fmMd/UZLjNnpsuxnzMbn+yUwLNc55PB+av87f9W/Mor1v5/rulWTH/N79i0xPi1q4cW61r2zyOXKVMW2JeXOFcW+JeXSFcW+5eXWV1+GYLOhcu8p4t8z8u5avmgcCAADAOtvXliACAAAsLAEMAABgEAEMAABgEAEMAABgEAEMAABgEAEMAABgEAEMAABgkP8Loavud7193tkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting word-count per review\n",
    "fig,(ax1,ax2)=plt.subplots(1,2, figsize=(15,8), gridspec_kw={'width_ratios': [10, 10]})\n",
    "\n",
    "train_words = data_pool[data_pool['score'] == 1]['word_count']\n",
    "ax1.hist(train_words,color='green')\n",
    "ax1.set_title('Positive Reviews')\n",
    "\n",
    "train_words=data_pool[data_pool['score'] == -1]['word_count']\n",
    "ax2.hist(train_words,color='red')\n",
    "ax2.set_title('Negative Reviews')\n",
    "\n",
    "fig.suptitle('Words per review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23758d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# data_pool.shape\n",
    "# data_pool.size\n",
    "# import sys\n",
    "# sys.getsizeof(data_pool)\n",
    "# data_pool.memory_usage()\n",
    "\n",
    "# dill.dump_session('notebook_env_abc.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50584e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    5595054\n",
      "-1    1328597\n",
      "Name: score, dtype: int64\n",
      "Total time to load clean data= 0.16547107696533203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "x = data_pool['score'].value_counts()\n",
    "print(x)\n",
    "sns.barplot(x.index,x)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time to load clean data=\",(end_time-start_time))\n",
    "\n",
    "# dill.dump_session('notebook_env_data_bar_plot.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7cbba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import STOPWORDS\n",
    "# text = \" \".join(text for text in data_pool['clean_text'])\n",
    "\n",
    "# #Create wordcloud\n",
    "# wordcloud = WordCloud(max_words=500, colormap=\"Set3\", background_color=\"white\").generate(text)\n",
    "# plt.figure( figsize=(15,10))\n",
    "# plt.imshow(wordcloud, interpolation='Bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71389df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to conver word 2 vector= 870.5241901874542\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() #Start time to calculate processing time\n",
    "# create Word2vec model\n",
    "#here words_f should be a list containing words from each document. say 1st row of the list is words from the 1st document/sentence\n",
    "#length of words_f is number of documents/sentences in your dataset\n",
    "data_pool['clean_text_tok']=[nltk.word_tokenize(i) for i in data_pool['clean_text']] #convert preprocessed sentence to tokenized sentence\n",
    "model = Word2Vec(data_pool['clean_text_tok'],min_count=1)  #min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  #combination of word and its vector\n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to conver word 2 vector=\",(end_time-start_time))\n",
    "\n",
    "# dill.dump_session('notebook_env_word2vec.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8377c273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to split dataset= 19.30233645439148\n",
      "Total time to tokenize dataset and= 1145.3329977989197\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "\n",
    "# dill.load_session('notebook_env_word2vec.db')\n",
    "\n",
    "start_time = time.time() \n",
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Input: \"reviewText\", \"rating\" and \"time\"\n",
    "# Target: \"log_votes\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_pool[\"clean_text\"],\n",
    "                                                  data_pool[\"score\"],\n",
    "                                                  test_size=0.2,\n",
    "                                             shuffle=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to split dataset=\",(end_time-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec\n",
    "X_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to tokenize dataset and=\",(end_time-start_time))\n",
    "\n",
    "# dill.dump_session('notebook_env_split_data.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e5bfb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to vectorize using tfid= 80.50812864303589\n",
      "hello\n",
      "Total time to vectorize using mean embedding= 370.10459780693054\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "#TF-IDF\n",
    "# Convert x_train to vector since model can only run on numbers and not words- Fit and transform\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) #tfidf runs on non-tokenized sentences unlike word2vec\n",
    "\n",
    "# Only transform x_test (not fit and transform)\n",
    "X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val) #Don't fit() your TfidfVectorizer to your test data: it will \n",
    "#change the word-indexes & weights to match test data. Rather, fit on the training data, then use the same train-data-\n",
    "#fit model on the test data, to reflect the fact you're analyzing the test data only based on what was learned without \n",
    "#it, and the have compatible\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to vectorize using tfid=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "start_time = time.time() \n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_val_vectors_w2v = modelw.transform(X_val_tok)\n",
    "print(\"hello\")\n",
    "# print(X_train_vectors_tfidf)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to vectorize using mean embedding=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "# dill.dump_session('notebook_env_tfid_vector.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "135e0acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to train logistic regression= 431.64828872680664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.63      0.69    265930\n",
      "           1       0.91      0.96      0.94   1118801\n",
      "\n",
      "    accuracy                           0.89   1384731\n",
      "   macro avg       0.85      0.79      0.82   1384731\n",
      "weighted avg       0.89      0.89      0.89   1384731\n",
      "\n",
      "Confusion Matrix: [[ 166236   99694]\n",
      " [  46469 1072332]]\n",
      "AUC: 0.924820971242493\n",
      "Total time to predict using logistic regression= 6.421611309051514\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "\n",
    "# print(X_train_vectors_tfidf)\n",
    "# print(y_train)\n",
    "    \n",
    "\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to train logistic regression=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "start_time = time.time() \n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_val_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob, pos_label=1)\n",
    "# roc_auc = roc_auc_score(y_val, y_prob, multi_class='ovo')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "print('AUC:', roc_auc)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to predict using logistic regression=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "# dill.dump_session('notebook_env_log_reg.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd554815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to train naive bayes= 0.9666252136230469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.30      0.45    265930\n",
      "           1       0.86      0.99      0.92   1118801\n",
      "\n",
      "    accuracy                           0.85   1384731\n",
      "   macro avg       0.85      0.64      0.68   1384731\n",
      "weighted avg       0.85      0.85      0.83   1384731\n",
      "\n",
      "Confusion Matrix: [[  80656  185274]\n",
      " [  15752 1103049]]\n",
      "AUC: 0.8923378635733358\n",
      "Total time to predict using Naive Bayes= 2.4828872680664062\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "#It's a probabilistic classifier that makes use of Bayes' Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.\n",
    "\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to train naive bayes=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_val_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to predict using Naive Bayes=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "# dill.dump_session('notebook_env_naive_bae.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f40b705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to train logistic regression w2v= 200.61376881599426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.47      0.58    265930\n",
      "           1       0.88      0.96      0.92   1118801\n",
      "\n",
      "    accuracy                           0.87   1384731\n",
      "   macro avg       0.82      0.72      0.75   1384731\n",
      "weighted avg       0.86      0.87      0.86   1384731\n",
      "\n",
      "Confusion Matrix: [[ 125691  140239]\n",
      " [  42080 1076721]]\n",
      "AUC: 0.8895365083116084\n",
      "Total time to predict using Logistic Regression with w2v= 13.067370176315308\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to train logistic regression w2v=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_val_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc) \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to predict using Logistic Regression with w2v=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "# dill.dump_session('notebook_env_log_w2v.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using SVM(tf-idf)\n",
    "\n",
    "svm_tfidf = SVC(solver = 'liblinear')\n",
    "svm_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to train svn=\",(end_time-start_time))\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = svm_tfidf.predict(X_val_vectors_tfidf)\n",
    "y_prob = svm_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to predict using SVM=\",(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Host model\n",
    "#joblib.dump(final_model, 'model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
