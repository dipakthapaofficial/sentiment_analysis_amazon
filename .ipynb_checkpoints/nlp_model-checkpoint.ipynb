{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748c5f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets\n",
    "\n",
    "from file_parser import getFiles, getCsvFiles\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import time\n",
    "import dill\n",
    "\n",
    "#Import joblib to host ml model\n",
    "# import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7726f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeScore(data):\n",
    "    if data < 3:\n",
    "        return -1\n",
    "    elif data > 3:\n",
    "        return 1\n",
    "    elif data == 3:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fb326a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(fileName):\n",
    "#     df_train_dataset = pd.read_json(fileName, lines = True)\n",
    "    df_train_dataset = pd.read_csv(fileName)\n",
    "    print(df_train_dataset.shape)\n",
    "    \n",
    "    #remove rows with empty reviews\n",
    "    df_train_dataset = df_train_dataset[df_train_dataset['reviewText'].notna()]\n",
    "    df_train_dataset = df_train_dataset[df_train_dataset['reviewText'] != \"\"]\n",
    "    df_train_dataset['overall'] = df_train_dataset['overall'].astype(object) # fix datatype error\n",
    "    \n",
    "    df_train_dataset['reviewText'] = df_train_dataset['reviewText'].astype(object) # fix datatype error\n",
    "    \n",
    "    #Add only useful data from dataset\n",
    "    dataset = {\"reviewText\": df_train_dataset[\"reviewText\"], \"overall\": df_train_dataset[\"overall\"]  }\n",
    "    \n",
    "    df_train_dataset = pd.DataFrame(data = dataset)\n",
    "    df_train_dataset = df_train_dataset.dropna()\n",
    "    \n",
    "    print(df_train_dataset.shape)\n",
    "\n",
    "#     df_train_dataset['score'] = df_train_dataset[\"overall\"].apply(lambda x: computeScore(x))\n",
    "\n",
    "    #Filter out neutral i.e. rating 3 from the list to segregate between positive and negative reviews\n",
    "    df_train_dataset[\"score\"] = df_train_dataset[\"overall\"].apply(lambda rating : +1 if str(rating) > '3' else -1)\n",
    "\n",
    "    #Get shape of the data frame\n",
    "    print(df_train_dataset.shape)\n",
    "\n",
    "    #Print data frame from top\n",
    "    print(df_train_dataset.head())\n",
    "    \n",
    "    #df_train_dataset.to_csv(fileName, index=False)\n",
    "    return df_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11767a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/Gift_Cards.json']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_pool = pd.DataFrame()  \n",
    "    \n",
    "    \n",
    "fileNames = getFiles(\"data\")\n",
    "# fileNames = getCsvFiles(\"data/final\")\n",
    "\n",
    "for fileName in fileNames:\n",
    "    dataset = classifier(fileName)\n",
    "    data_pool = data_pool.append(dataset, ignore_index = True)\n",
    "\n",
    "data_pool.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a317d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = data_pool['score'].value_counts()\n",
    "print(x)\n",
    "sns.barplot(x.index,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pool.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec6bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. WORD-COUNT\n",
    "data_pool['word_count'] = data_pool['reviewText'].apply(lambda x: len(str(x).split()))\n",
    "print(data_pool[data_pool['score'] == -1]['word_count'].mean()) #Negative Reviews\n",
    "print(data_pool[data_pool['score'] == 1]['word_count'].mean()) #Positive Reviews\n",
    "# print(data_pool[data_pool['score'] == 0]['word_count'].mean()) #Neutral Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. CHARACTER-COUNT\n",
    "data_pool['char_count'] = data_pool['reviewText'].apply(lambda x: len(str(x)))\n",
    "print(data_pool[data_pool['score'] == -1]['char_count'].mean()) #Negative Reviews\n",
    "print(data_pool[data_pool['score'] == 1]['char_count'].mean()) #Positive Reviews\n",
    "# print(data_pool[data_pool['score'] == 0]['char_count'].mean()) #Neutral Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98391278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. UNIQUE WORD-COUNT\n",
    "data_pool['unique_word_count'] = data_pool['reviewText'].apply(lambda x: len(set(str(x).split())))\n",
    "print(data_pool[data_pool['score'] == -1]['unique_word_count'].mean()) #Negative Reviews\n",
    "print(data_pool[data_pool['score'] == 1]['unique_word_count'].mean()) #Positive Reviews\n",
    "# print(data_pool[data_pool['score'] == 0]['unique_word_count'].mean()) #Neutral Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting word-count per review\n",
    "fig,(ax1,ax2)=plt.subplots(1, 2, figsize=(500,100))\n",
    "\n",
    "train_words = data_pool[data_pool['score'] == 1]['word_count']\n",
    "ax1.hist(train_words,color='green')\n",
    "ax1.set_title('Positive Reviews')\n",
    "\n",
    "train_words=data_pool[data_pool['score'] == -1]['word_count']\n",
    "ax2.hist(train_words,color='red')\n",
    "ax2.set_title('Negative Reviews')\n",
    "\n",
    "# train_words=data_pool[data_pool['score'] == 0]['word_count']\n",
    "#ax3.hist(train_words,color='blue')\n",
    "#ax3.set_title('Neutral Reviews')\n",
    "\n",
    "fig.suptitle('Words per review')\n",
    "plt.show()\n",
    "\n",
    "# fig, ax3 = plt.subplots(1, 2)\n",
    "# train_words=data_pool[data_pool['score'] == 0]['word_count']\n",
    "# ax3.hist(train_words, color='blue')\n",
    "# ax3.set_title('Neutral Reviews')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01d48d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned it may involve some things like adjacent spaces and tabs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#1. Common text preprocessing\n",
    "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\n",
    "\n",
    "#convert to lowercase and remove punctuations and characters and then strip\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = text.lower()#lowercase text\n",
    "    text=text.strip() #get rid of leading/trailing whitespace \n",
    "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful\n",
    "    text = re.sub('\\s+', ' ', text)   #Remove extra space and tabs\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)   #[0-9] matches any digit (0 to 10000...)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip()) \n",
    "    text = re.sub(r'\\d',' ',text)  #matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text)   #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n",
    "    \n",
    "    return text\n",
    "\n",
    "text=preprocess(text)\n",
    "print(text)  #text is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390fc810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messag clean may involv thing like adjac space tab\n",
      "messag clean may involv thing like adjac space tab\n",
      "messag clean may involv thing like adjac space tab\n"
     ]
    }
   ],
   "source": [
    "#3. LEXICON-BASED TEXT PROCESSING EXAMPLES\n",
    " \n",
    "#1. STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "text=stopword(text)\n",
    "print(text)\n",
    "\n",
    "#2. STEMMING\n",
    " \n",
    "# Initialize the stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "def stemming(string):\n",
    "    a=[snow.stem(i) for i in word_tokenize(string) ]\n",
    "    return \" \".join(a)\n",
    "text=stemming(text)\n",
    "print(text)\n",
    "\n",
    "#3. LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "# def lemmatizer(string, cores=6):  \n",
    "#     a = []\n",
    "    \n",
    "#     with Pool(processes=cores) as pool:\n",
    "#         # Initialize the lemmatizer\n",
    "#         wl = WordNetLemmatizer()\n",
    "#         word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "#         a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    \n",
    "#     return \" \".join(a)\n",
    "\n",
    "text = lemmatizer(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL PREPROCESSING\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "    return stopword(preprocess(string))\n",
    "\n",
    "# data_pool=data_pool.drop(columns=['word_count','char_count','unique_word_count'])\n",
    "data_pool['clean_text'] = data_pool['reviewText'].apply(lambda x: finalpreprocess(x))\n",
    "data_pool.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eaaf0a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Export cleaned up data-pool for future usage. skipping previous steps\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdata_pool\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_up_data_jupyter_with_clean_text.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_pool' is not defined"
     ]
    }
   ],
   "source": [
    "#Export cleaned up data-pool for future usage. skipping previous steps\n",
    "data_pool.to_csv('cleaned_up_data_jupyter_with_clean_text.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4874bda7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/final/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m data_pool \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m---> 10\u001b[0m fileNames \u001b[38;5;241m=\u001b[39m \u001b[43mgetCsvFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/final/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileName \u001b[38;5;129;01min\u001b[39;00m fileNames:\n\u001b[1;32m     12\u001b[0m     data_pool_classified \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(fileName)\n",
      "File \u001b[0;32m~/Downloads/All_Amazon_Review/code/file_parser.py:13\u001b[0m, in \u001b[0;36mgetCsvFiles\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetCsvFiles\u001b[39m(location):\n\u001b[0;32m---> 13\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     filesToBeRead \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#Ignore all files except json files\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/final/'"
     ]
    }
   ],
   "source": [
    "# data_pool.head()\n",
    "# fileNames = getCsvFiles(\"final\")\n",
    "# data_pool = pd.DataFrame()\n",
    "# for fileName in fileNames:\n",
    "#     data_pool_classified = pd.read_csv(fileName)\n",
    "#     data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
    "\n",
    "start_time = time.time()\n",
    "data_pool = pd.DataFrame()\n",
    "fileNames = getCsvFiles('../data/final/')\n",
    "for fileName in fileNames:\n",
    "    data_pool_classified = pd.read_csv(fileName)\n",
    "    data_pool = data_pool.append(data_pool_classified, ignore_index = True)\n",
    "    \n",
    "\n",
    "print(data_pool.shape)\n",
    "data_pool.head()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time to load clean data=\",(end_time-start_time))\n",
    "\n",
    "dill.dump_session('notebook_env_data_load.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1466ef22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewText    0\n",
       "overall       0\n",
       "score         0\n",
       "clean_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pool.isna().sum()\n",
    "\n",
    "data_pool = data_pool[data_pool['clean_text'].notna()]\n",
    "data_pool = data_pool[data_pool['reviewText'].notna()]\n",
    "\n",
    "data_pool.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23758d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# data_pool.shape\n",
    "# data_pool.size\n",
    "# import sys\n",
    "# sys.getsizeof(data_pool)\n",
    "# data_pool.memory_usage()\n",
    "\n",
    "# dill.dump_session('notebook_env_abc.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50584e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    7489282\n",
      "-1    1993478\n",
      "Name: score, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load clean data= 0.49791526794433594\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "x = data_pool['score'].value_counts()\n",
    "print(x)\n",
    "sns.barplot(x.index,x)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time to load clean data=\",(end_time-start_time))\n",
    "\n",
    "dill.dump_session('notebook_env_data_bar_plot.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7cbba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import STOPWORDS\n",
    "# text = \" \".join(text for text in data_pool['clean_text'])\n",
    "\n",
    "# #Create wordcloud\n",
    "# wordcloud = WordCloud(max_words=500, colormap=\"Set3\", background_color=\"white\").generate(text)\n",
    "# plt.figure( figsize=(15,10))\n",
    "# plt.imshow(wordcloud, interpolation='Bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71389df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to conver word 2 vector= 2647.7900133132935\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() #Start time to calculate processing time\n",
    "# create Word2vec model\n",
    "#here words_f should be a list containing words from each document. say 1st row of the list is words from the 1st document/sentence\n",
    "#length of words_f is number of documents/sentences in your dataset\n",
    "data_pool['clean_text_tok']=[nltk.word_tokenize(i) for i in data_pool['clean_text']] #convert preprocessed sentence to tokenized sentence\n",
    "model = Word2Vec(data_pool['clean_text_tok'],min_count=1)  #min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  #combination of word and its vector\n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to conver word 2 vector=\",(end_time-start_time))\n",
    "\n",
    "dill.dump_session('notebook_env_word2vec.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8377c273",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CorpusReader' object has no attribute '_unload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdill\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_session\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnotebook_env_word2vec.db\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\u001b[39;00m\n\u001b[1;32m      7\u001b[0m  \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Input: \"reviewText\", \"rating\" and \"time\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Target: \"log_votes\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/dill/_dill.py:520\u001b[0m, in \u001b[0;36mload_session\u001b[0;34m(filename, main, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m unpickler\u001b[38;5;241m.\u001b[39m_main \u001b[38;5;241m=\u001b[39m main\n\u001b[1;32m    519\u001b[0m unpickler\u001b[38;5;241m.\u001b[39m_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m unpickler\u001b[38;5;241m.\u001b[39m_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    522\u001b[0m main\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/dill/_dill.py:646\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;66;03m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mStockUnpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_main_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore:\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# point obj class to main\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CorpusReader' object has no attribute '_unload'"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "\n",
    "dill.load_session('notebook_env_word2vec.db')\n",
    "\n",
    "start_time = time.time() \n",
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Input: \"reviewText\", \"rating\" and \"time\"\n",
    "# Target: \"log_votes\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_pool[\"clean_text\"],\n",
    "                                                  data_pool[\"score\"],\n",
    "                                                  test_size=0.2,\n",
    "                                             shuffle=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to split dataset=\",(end_time-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec\n",
    "X_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to tokenize dataset and=\",(end_time-start_time))\n",
    "\n",
    "dill.dump_session('notebook_env_split_data.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5bfb18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime() \n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#TF-IDF\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert x_train to vector since model can only run on numbers and not words- Fit and transform\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "#TF-IDF\n",
    "# Convert x_train to vector since model can only run on numbers and not words- Fit and transform\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) #tfidf runs on non-tokenized sentences unlike word2vec\n",
    "\n",
    "# Only transform x_test (not fit and transform)\n",
    "X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val) #Don't fit() your TfidfVectorizer to your test data: it will \n",
    "#change the word-indexes & weights to match test data. Rather, fit on the training data, then use the same train-data-\n",
    "#fit model on the test data, to reflect the fact you're analyzing the test data only based on what was learned without \n",
    "#it, and the have compatible\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to vectorize using tfid=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "start_time = time.time() \n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_val_vectors_w2v = modelw.transform(X_val_tok)\n",
    "print(\"hello\")\n",
    "# print(X_train_vectors_tfidf)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to vectorize using mean embedding=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "dill.dump_session('notebook_env_tfid_vector.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e0acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "\n",
    "# print(X_train_vectors_tfidf)\n",
    "# print(y_train)\n",
    "    \n",
    "\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_val_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob, pos_label=1)\n",
    "# roc_auc = roc_auc_score(y_val, y_prob, multi_class='ovo')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "print('AUC:', roc_auc)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to train and predict using logistic regression=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "dill.dump_session('notebook_env_log_reg.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd554815",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "#It's a probabilistic classifier that makes use of Bayes' Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.\n",
    "\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_val_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to train and predict using Naive Bayes=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "dill.dump_session('notebook_env_naive_bae.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_val_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc) \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to train and predict using Logistic Regression with w2v=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "dill.dump_session('notebook_env_log_w2v.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(w2v)\n",
    "#It's a probabilistic classifier that makes use of Bayes' Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.\n",
    "\n",
    "multi = MultinomialNB()\n",
    "multi.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = multi.predict(X_val_vectors_w2v)\n",
    "y_prob = multi.predict_proba(X_val_vectors_w2v)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to train and predict using Naive Bayes=\",(end_time-start_time)\n",
    "\n",
    "\n",
    "dill.dump_session('notebook_env_naive_w2v.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "# #FITTING THE CLASSIFICATION MODEL using SVN (W2v)\n",
    "# lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "# lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "# #Predict y value for test dataset\n",
    "# y_predict = lr_w2v.predict(X_val_vectors_w2v)\n",
    "# y_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n",
    " \n",
    "\n",
    "# print(classification_report(y_val,y_predict))\n",
    "# print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "# fpr, tpr, thresholds = roc_curve(y_val, y_prob, pos_label=1)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# print('AUC:', roc_auc) \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time to train and predict using Logistic Regression with w2v=\",(end_time-start_time))\n",
    "\n",
    "\n",
    "dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Host model\n",
    "#joblib.dump(final_model, 'model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
